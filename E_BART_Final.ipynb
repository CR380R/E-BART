{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E-BART_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mYEFYRir0XVi",
        "JEFS_4OW0klv",
        "fUMM4Mak066h",
        "gW9cO5rj1prD",
        "tZVGcVs57qhR",
        "ztpzu6hU13OD",
        "E-hK7obY2Qs6",
        "4ii_khjb2wzY",
        "_ImO7LQk31w9",
        "6gFLzB384tzX",
        "3PLzks2E5IK_",
        "8hyRlNgD5WZ2",
        "yPYyJLTK5j2Q"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRTNJOUt8bdu"
      },
      "source": [
        "# E-BART System Definition\n",
        "# Author: Erik Brand, UQ\n",
        "# Last Updated: 3/12/2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOoozbcZ7OuM"
      },
      "source": [
        "README:\n",
        "\n",
        "To Train: Run sections 1, 2, 3, 4, 5\n",
        "\n",
        "To Run Inference: Run sections 1, 2, 3, 4, 6, 7 \n",
        "\n",
        "To Run Inference With Temperature Scaling: Run sections 1, 2, 3, 4, 6, 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYEFYRir0XVi"
      },
      "source": [
        "# 1. Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y36sbA2t0QJ6"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!git checkout v4.10.3\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAqqQL4r0djw"
      },
      "source": [
        "!pip install datasets==1.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pACw0K5t1B_7"
      },
      "source": [
        "!pip install rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEFS_4OW0klv"
      },
      "source": [
        "# 2. E-BART Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "_g1JKY7U0nQl",
        "outputId": "317855dc-4c5e-4fc4-f3bc-435aa32279b3"
      },
      "source": [
        "from transformers import BartTokenizer, BartModel, BartPretrainedModel, BartConfig, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers.models.bart.modeling_bart import BartClassificationHead, shift_tokens_right\n",
        "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
        "from transformers.generation_logits_process import LogitsProcessorList, MinLengthLogitsProcessor\n",
        "from transformers.generation_utils import GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput\n",
        "from transformers.file_utils import ModelOutput\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df08dcc6b866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartPretrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartClassificationHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_tokens_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeq2SeqLMOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_logits_process\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinLengthLogitsProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGreedySearchOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSampleOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSampleOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQNp3qs0oKd"
      },
      "source": [
        "class Seq2SeqJointOutput(ModelOutput):\n",
        "  classification_logits: torch.FloatTensor = None\n",
        "  loss: Optional[torch.FloatTensor] = None\n",
        "  logits: torch.FloatTensor = None\n",
        "  past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "  decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
        "  encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmRi5ZZH0qy8"
      },
      "source": [
        "class BartForJointPrediction(BartPretrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        self.model = BartModel(config)\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "        self.classification_head = BartClassificationHead(\n",
        "            config.d_model,\n",
        "            config.d_model,\n",
        "            config.num_labels,  # Defaults to 3 in BART config\n",
        "            config.classifier_dropout,\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self._resize_final_logits_bias(new_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        classification_labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        # \"\"\"\n",
        "        # labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "        #     Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n",
        "        #     config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n",
        "        #     (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "        # Returns:\n",
        "        # \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if decoder_input_ids is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "        \n",
        "        # print(\"INPUTS\")\n",
        "        # print(input_ids.shape)\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Summarization Logits\n",
        "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
        "\n",
        "\n",
        "        # Classification Logits\n",
        "        hidden_states = outputs[0]  # last hidden state\n",
        "\n",
        "        classification_logits = None\n",
        "        if input_ids is not None:   # In all cases except generating autoregressively - we don't want to produce classficiation logits\n",
        "            # Can't just use decoder_input_ids all the time as these are already shifted right - might not have ending eos_token_id <s>\n",
        "            if labels is not None:\n",
        "                # Training\n",
        "                eos_mask = labels.eq(self.config.eos_token_id)\n",
        "            else:\n",
        "                # Inference\n",
        "                # We want this to match what happens in the training task: \n",
        "                # labels are shifted right and passed to the decoder\n",
        "                # The decoder predicts eos_token_id based on the last token in label (that is not eos_token_id as this has been shifted off)\n",
        "                # For inference we pass in final predicted summary, make classification prediction based on last token before eos_token_id, just like training\n",
        "                eos_mask = decoder_input_ids.eq(self.config.eos_token_id)\n",
        "                # Shift mask left so that True lines up with token immediately before eos_token_id <s>\n",
        "                shifted_mask = eos_mask.new_zeros(eos_mask.shape)\n",
        "                shifted_mask[:, :-1] = eos_mask[:, 1:].clone()\n",
        "                shifted_mask[:, -1] = False\n",
        "                eos_mask = shifted_mask\n",
        "         \n",
        "            \n",
        "            if len(torch.unique(eos_mask.sum(1))) > 1:\n",
        "                # print(decoder_input_ids)\n",
        "                raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n",
        "            sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n",
        "                :, -1, :\n",
        "            ]\n",
        "            classification_logits = self.classification_head(sentence_representation)\n",
        "\n",
        "\n",
        "        # Summarization Loss\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        # Classification Loss\n",
        "        classification_loss = None\n",
        "        if classification_labels is not None:\n",
        "            if self.config.num_labels == 1:\n",
        "                # regression\n",
        "                loss_fct = MSELoss()\n",
        "                classification_loss = loss_fct(classification_logits.view(-1), classification_labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                classification_loss = loss_fct(classification_logits.view(-1, self.config.num_labels), classification_labels.view(-1))\n",
        "\n",
        "        loss = None\n",
        "        if (labels is not None) and (classification_labels is not None):\n",
        "            # Joint loss\n",
        "            loss = 0.5 * masked_lm_loss + 0.5 * classification_loss\n",
        "        elif labels is not None:\n",
        "            # Only summarisation loss\n",
        "            loss = masked_lm_loss\n",
        "        elif classification_labels is not None:\n",
        "            # Only classification loss\n",
        "            loss = classification_loss\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits) + outputs[1:] + (classification_logits)\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return Seq2SeqJointOutput(\n",
        "            loss=loss,\n",
        "            logits=lm_logits,\n",
        "            classification_logits=classification_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"decoder_head_mask\": decoder_head_mask,\n",
        "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past\n",
        "    \n",
        "\n",
        "\n",
        "    # Override greedy_search from generate MixIn:\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        max_length: Optional[int] = None,\n",
        "        min_length: Optional[int] = None,\n",
        "        do_sample: Optional[bool] = None,\n",
        "        early_stopping: Optional[bool] = None,\n",
        "        num_beams: Optional[int] = None,\n",
        "        temperature: Optional[float] = None,\n",
        "        top_k: Optional[int] = None,\n",
        "        top_p: Optional[float] = None,\n",
        "        repetition_penalty: Optional[float] = None,\n",
        "        bad_words_ids: Optional[Iterable[int]] = None,\n",
        "        bos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        length_penalty: Optional[float] = None,\n",
        "        no_repeat_ngram_size: Optional[int] = None,\n",
        "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
        "        num_return_sequences: Optional[int] = None,\n",
        "        max_time: Optional[float] = None,\n",
        "        max_new_tokens: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        num_beam_groups: Optional[int] = None,\n",
        "        diversity_penalty: Optional[float] = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        output_scores: Optional[bool] = None,\n",
        "        return_dict_in_generate: Optional[bool] = None,\n",
        "        forced_bos_token_id: Optional[int] = None,\n",
        "        forced_eos_token_id: Optional[int] = None,\n",
        "        remove_invalid_values: Optional[bool] = None,\n",
        "        synced_gpus: Optional[bool] = None,\n",
        "        **model_kwargs,\n",
        "    ) -> Union[GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput, torch.LongTensor]:\n",
        "        \n",
        "        # First (autoregressive) pass of model to generate summary\n",
        "        summarization_outputs = super().generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            do_sample=do_sample,\n",
        "            early_stopping=early_stopping,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            bad_words_ids=bad_words_ids,\n",
        "            bos_token_id=bos_token_id,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            length_penalty=length_penalty,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            max_time=max_time,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            decoder_start_token_id=decoder_start_token_id,\n",
        "            use_cache=use_cache,\n",
        "            num_beam_groups=num_beam_groups,\n",
        "            diversity_penalty=diversity_penalty,\n",
        "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=True,\n",
        "            forced_bos_token_id=forced_bos_token_id,\n",
        "            forced_eos_token_id=forced_eos_token_id,\n",
        "            remove_invalid_values=remove_invalid_values,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs)\n",
        "\n",
        "        \n",
        "        # Second pass of model to get classification output\n",
        "        classification_outputs = self(input_ids=input_ids,    # Do another pass of the encoder\n",
        "                                      decoder_input_ids=summarization_outputs['sequences'], # The final summary\n",
        "                                      use_cache=use_cache,\n",
        "                                      output_attentions=output_attentions,\n",
        "                                      output_hidden_states=output_hidden_states,\n",
        "                                      **model_kwargs)\n",
        "        \n",
        "        return (classification_outputs['classification_logits'], summarization_outputs['sequences'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUMM4Mak066h"
      },
      "source": [
        "# 3. Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqUCS8-d0-SX"
      },
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
        "from transformers.trainer_utils import IntervalStrategy, SchedulerType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW9cO5rj1prD"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS8B1Jdc1RNw"
      },
      "source": [
        "Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1FUF_O-0-2v"
      },
      "source": [
        "df_efever = pd.read_json(\"/content/drive/MyDrive/Thesis/System_Development/fever_data/efever_train_set.jsonl\",\n",
        "                  orient=\"columns\", lines=True)\n",
        "df_efever = df_efever.set_index(\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qHetRmt1HuJ"
      },
      "source": [
        "df_fever = pd.read_json(\"/content/drive/MyDrive/Thesis/System_Development/fever_data/train.jsonl\",\n",
        "                        orient=\"columns\", lines=True)\n",
        "df_fever = df_fever.set_index(\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTHgjxRP1KbD"
      },
      "source": [
        "df = pd.concat([df_fever, df_efever], axis=1, join=\"inner\")\n",
        "df = df.drop(columns=['evidence', 'verifiable'])\n",
        "# Convert labels to integer values\n",
        "df[\"label\"] = df[\"label\"].replace(to_replace={'SUPPORTS':0, 'REFUTES':1, 'NOT ENOUGH INFO':2}, value=None)\n",
        "# Remove + from retrieved_evidence\n",
        "df[\"retrieved_evidence\"] = df[\"retrieved_evidence\"].str.replace(\"+\", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp_x0BH31M5l"
      },
      "source": [
        "# To get rid of rows without enough evidence\n",
        "df = df[df['summary'] != '\"The relevant information about the claim is lacking in the context.\"']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF7aVZRx1P1w"
      },
      "source": [
        "train_dataset = Dataset.from_pandas(df)\n",
        "train_dataset = train_dataset.remove_columns(['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtXN_B0z1VnA"
      },
      "source": [
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neECqZ6Q1Spr"
      },
      "source": [
        "Validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTqqM29t1Tkj"
      },
      "source": [
        "df_efever_val = pd.read_json(\"/content/drive/MyDrive/Thesis/System_Development/fever_data/efever_dev_set.jsonl\",\n",
        "                  orient=\"columns\", lines=True)\n",
        "df_efever_val = df_efever_val.set_index(\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0WAQ_2l1ZDn"
      },
      "source": [
        "df_fever_val = pd.read_json(\"/content/drive/MyDrive/Thesis/System_Development/fever_data/dev.jsonl\",\n",
        "                        orient=\"columns\", lines=True)\n",
        "df_fever_val = df_fever_val.set_index(\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuyqDKBd1a3T"
      },
      "source": [
        "df_val = pd.concat([df_fever_val, df_efever_val], axis=1, join=\"inner\")\n",
        "df_val = df_val.drop(columns=['evidence', 'verifiable'])\n",
        "# Convert labels to integer values\n",
        "df_val[\"label\"] = df_val[\"label\"].replace(to_replace={'SUPPORTS':0, 'REFUTES':1, 'NOT ENOUGH INFO':2}, value=None)\n",
        "# Remove + from retrieved_evidence\n",
        "df_val[\"retrieved_evidence\"] = df_val[\"retrieved_evidence\"].str.replace(\"+\", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OwpIdNC1crv"
      },
      "source": [
        "# To get rid of rows without enough evidence\n",
        "df_val = df_val[df_val['summary'] != '\"The relevant information about the claim is lacking in the context.\"']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEoXYtfw1ew1"
      },
      "source": [
        "val_dataset = Dataset.from_pandas(df_val)\n",
        "val_dataset = val_dataset.remove_columns(['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W90fONM-1ggl"
      },
      "source": [
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbVu20fo1kwl"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAmi-yKy1jzn"
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqqhw6R71sft"
      },
      "source": [
        "def preprocess(data):\n",
        "  claim = data['claim']\n",
        "  evidence = data['retrieved_evidence']\n",
        "  summary = data['summary']\n",
        "  label = data['label']\n",
        "\n",
        "  model_inputs = tokenizer(claim, evidence, max_length=1024, truncation=True, padding=False)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    summarization_labels = tokenizer(summary, max_length=128, truncation=True, padding=False)\n",
        "\n",
        "  # Ensure padding not included in loss\n",
        "  summarization_labels[\"input_ids\"] = [\n",
        "      [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in summarization_labels[\"input_ids\"]\n",
        "  ]\n",
        "\n",
        "  model_inputs['classification_labels'] = label   # This doesn't require one-hot encoding because of the way pytorch CrossEntropy works\n",
        "  model_inputs['labels'] = summarization_labels['input_ids']\n",
        "\n",
        "  return model_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S1cJ5Wz1vtI"
      },
      "source": [
        "train_dataset = train_dataset.map(\n",
        "                preprocess,\n",
        "                batched=True,\n",
        "                num_proc=None,\n",
        "                remove_columns=train_dataset.column_names,\n",
        "                load_from_cache_file=True,\n",
        "                desc=\"Running tokenizer on train dataset\",\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33z_a2Xo1xky"
      },
      "source": [
        "val_dataset = val_dataset.map(\n",
        "                preprocess,\n",
        "                batched=True,\n",
        "                num_proc=None,\n",
        "                remove_columns=val_dataset.column_names,\n",
        "                load_from_cache_file=True,\n",
        "                desc=\"Running tokenizer on val dataset\",\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZVGcVs57qhR"
      },
      "source": [
        "# 4. Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h9mAXnk7wV0"
      },
      "source": [
        "# Data collator\n",
        "label_pad_token_id = -100\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUmhPDrP7ttp"
      },
      "source": [
        "# Metric\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # rougeLSum expects newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if True: #data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results from ROUGE\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztpzu6hU13OD"
      },
      "source": [
        "# 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ASWgXV17X_"
      },
      "source": [
        "model = BartForJointPrediction.from_pretrained('facebook/bart-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cunlPWh_1936"
      },
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    adafactor=False,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    adam_epsilon=1e-08,\n",
        "    dataloader_drop_last=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=True,\n",
        "    ddp_find_unused_parameters=None,\n",
        "    debug=[],\n",
        "    deepspeed=None,\n",
        "    disable_tqdm=False,\n",
        "    do_eval=True,\n",
        "    do_predict=False,\n",
        "    do_train=True,\n",
        "    eval_accumulation_steps=None,\n",
        "    eval_steps=500,\n",
        "    evaluation_strategy='no',\n",
        "    fp16=False,\n",
        "    fp16_backend='auto',\n",
        "    fp16_full_eval=False,\n",
        "    fp16_opt_level='O1',\n",
        "    gradient_accumulation_steps=1,\n",
        "    greater_is_better=None,\n",
        "    group_by_length=False,\n",
        "    ignore_data_skip=False,\n",
        "    label_names=None,\n",
        "    label_smoothing_factor=0.0,\n",
        "    learning_rate=5e-05,\n",
        "    length_column_name='length',\n",
        "    load_best_model_at_end=False,\n",
        "    local_rank=-1,\n",
        "    # log_level=-1,\n",
        "    # log_level_replica=-1,\n",
        "    log_on_each_node=True,\n",
        "    logging_dir='/tmp/tst-summarization/runs/Jul04_02-41-44_9ee3aa777e7a',\n",
        "    logging_first_step=False,\n",
        "    logging_steps=500,\n",
        "    logging_strategy='steps',\n",
        "    lr_scheduler_type='linear',\n",
        "    max_grad_norm=1.0,\n",
        "    max_steps=-1,\n",
        "    metric_for_best_model=None,\n",
        "    # mp_parameters=,\n",
        "    no_cuda=False,\n",
        "    num_train_epochs=3.0,\n",
        "    output_dir='/tmp/tst-summarization',\n",
        "    overwrite_output_dir=True,\n",
        "    past_index=-1,\n",
        "    per_device_eval_batch_size=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    predict_with_generate=True,\n",
        "    prediction_loss_only=False,\n",
        "    push_to_hub=False,\n",
        "    push_to_hub_model_id='tst-summarization',\n",
        "    push_to_hub_organization=None,\n",
        "    push_to_hub_token=None,\n",
        "    remove_unused_columns=True,\n",
        "    report_to=['tensorboard'],\n",
        "    resume_from_checkpoint=None,\n",
        "    run_name='/tmp/tst-summarization',\n",
        "    save_on_each_node=False,\n",
        "    save_steps=10000,\n",
        "    save_strategy='steps',\n",
        "    save_total_limit=None,\n",
        "    seed=42,\n",
        "    sharded_ddp=[],\n",
        "    skip_memory_metrics=True,\n",
        "    sortish_sampler=False,\n",
        "    tpu_metrics_debug=False,\n",
        "    tpu_num_cores=None,\n",
        "    use_legacy_prediction_loop=False,\n",
        "    warmup_ratio=0.0,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VH0jgy-2FL9"
      },
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EogNPPPu2HBx"
      },
      "source": [
        "train_result = trainer.train(resume_from_checkpoint=None)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-hK7obY2Qs6"
      },
      "source": [
        "# 6. Custom Trainer Definition (For Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATYxsYkT2T9R"
      },
      "source": [
        "from transformers import Trainer\n",
        "from transformers.utils import logging\n",
        "from typing import NamedTuple\n",
        "from transformers.file_utils import is_torch_tpu_available\n",
        "from transformers.trainer_pt_utils import (\n",
        "    DistributedTensorGatherer,\n",
        "    IterableDatasetShard,\n",
        "    SequentialDistributedSampler,\n",
        "    find_batch_size,\n",
        "    nested_concat,\n",
        "    nested_numpify,\n",
        "    nested_truncate,\n",
        ")\n",
        "from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n",
        "from transformers.trainer_utils import (\n",
        "    EvalLoopOutput,\n",
        "    EvalPrediction,\n",
        "    PredictionOutput,\n",
        "    denumpify_detensorize,\n",
        "    speed_metrics,\n",
        ")\n",
        "from transformers.debug_utils import DebugOption\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\n",
        "\n",
        "import time\n",
        "import math\n",
        "import collections\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dusn_QI52Wny"
      },
      "source": [
        "nltk.download(\"punkt\", quiet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1tnnkS2Y5g"
      },
      "source": [
        "logger = logging.get_logger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DnosOtF2alO"
      },
      "source": [
        "class JointPredictionOutput(NamedTuple):\n",
        "    predictions: Union[np.ndarray, Tuple[np.ndarray]]\n",
        "    label_ids: Optional[np.ndarray]\n",
        "    metrics: Optional[Dict[str, float]]\n",
        "    classification_predictions: Union[np.ndarray, Tuple[np.ndarray]]\n",
        "    classification_label_ids: Optional[np.ndarray]\n",
        "\n",
        "\n",
        "class JointEvalLoopOutput(NamedTuple):\n",
        "    predictions: Union[np.ndarray, Tuple[np.ndarray]]\n",
        "    label_ids: Optional[np.ndarray]\n",
        "    metrics: Optional[Dict[str, float]]\n",
        "    num_samples: Optional[int]\n",
        "    classification_predictions: Union[np.ndarray, Tuple[np.ndarray]]\n",
        "    classification_label_ids: Optional[np.ndarray]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJup2tPv2cbG"
      },
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def prediction_step(\n",
        "          self,\n",
        "          model: nn.Module,\n",
        "          inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "          prediction_loss_only: bool,\n",
        "          ignore_keys: Optional[List[str]] = None,\n",
        "      ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "          \"\"\"\n",
        "          Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
        "\n",
        "          Subclass and override to inject custom behavior.\n",
        "\n",
        "          Args:\n",
        "              model (:obj:`nn.Module`):\n",
        "                  The model to evaluate.\n",
        "              inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "                  The inputs and targets of the model.\n",
        "\n",
        "                  The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "                  argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
        "              prediction_loss_only (:obj:`bool`):\n",
        "                  Whether or not to return the loss only.\n",
        "\n",
        "          Return:\n",
        "              Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
        "              labels (each being optional).\n",
        "          \"\"\"\n",
        "\n",
        "          if not self.args.predict_with_generate or prediction_loss_only:\n",
        "              return super().prediction_step(\n",
        "                  model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n",
        "              )\n",
        "\n",
        "          has_labels = \"labels\" in inputs\n",
        "          inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "          # XXX: adapt synced_gpus for fairscale as well\n",
        "          gen_kwargs = {\n",
        "              \"max_length\": self._max_length if self._max_length is not None else self.model.config.max_length,\n",
        "              \"num_beams\": self._num_beams if self._num_beams is not None else self.model.config.num_beams,\n",
        "              \"synced_gpus\": True if is_deepspeed_zero3_enabled() else False,\n",
        "          }\n",
        "\n",
        "          result = self.model.generate(\n",
        "              inputs[\"input_ids\"],\n",
        "              attention_mask=inputs[\"attention_mask\"],\n",
        "              **gen_kwargs,\n",
        "          )\n",
        "\n",
        "          classification_logits = result[0]\n",
        "          generated_tokens = result[1]\n",
        "\n",
        "          # in case the batch is shorter than max length, the output should be padded\n",
        "          if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "              generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
        "\n",
        "          with torch.no_grad():\n",
        "              if self.use_amp:\n",
        "                  with autocast():\n",
        "                      outputs = model(**inputs)\n",
        "              else:\n",
        "                  outputs = model(**inputs)\n",
        "              if has_labels:\n",
        "                  if self.label_smoother is not None:\n",
        "                      loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
        "                  else:\n",
        "                      loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
        "              else:\n",
        "                  loss = None\n",
        "\n",
        "          if self.args.prediction_loss_only:\n",
        "              return (loss, None, None)\n",
        "\n",
        "          labels = inputs[\"labels\"]\n",
        "          classification_labels = inputs[\"classification_labels\"]\n",
        "          if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "              labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
        "\n",
        "          return (loss, generated_tokens, labels, classification_logits, classification_labels)\n",
        "\n",
        "\n",
        "\n",
        "    def evaluation_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str,\n",
        "        prediction_loss_only: Optional[bool] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> JointEvalLoopOutput:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by :obj:`Trainer.evaluate()` and :obj:`Trainer.predict()`.\n",
        "\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        prediction_loss_only = (\n",
        "            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only\n",
        "        )\n",
        "\n",
        "        # if eval is called w/o train init deepspeed here\n",
        "        if self.args.deepspeed and not self.deepspeed:\n",
        "\n",
        "            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
        "            # from the checkpoint eventually\n",
        "            deepspeed_engine, _, _ = deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)\n",
        "            self.model = deepspeed_engine.module\n",
        "            self.model_wrapped = deepspeed_engine\n",
        "            self.deepspeed = deepspeed_engine\n",
        "            # XXX: we don't need optim/sched for inference, but this needs to be sorted out, since\n",
        "            # for example the Z3-optimizer is a must for zero3 to work even for inference - what we\n",
        "            # don't need is the deepspeed basic optimizer which is self.optimizer.optimizer\n",
        "            deepspeed_engine.optimizer.optimizer = None\n",
        "            deepspeed_engine.lr_scheduler = None\n",
        "\n",
        "        model = self._wrap_model(self.model, training=False)\n",
        "\n",
        "        # if full fp16 is wanted on eval and this ``evaluation`` or ``predict`` isn't called while\n",
        "        # ``train`` is running, halve it first and then put on device\n",
        "        if not self.is_in_train and self.args.fp16_full_eval:\n",
        "            model = model.half().to(self.args.device)\n",
        "\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        logger.info(f\"***** Running {description} *****\")\n",
        "        if isinstance(dataloader.dataset, collections.abc.Sized):\n",
        "            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n",
        "        else:\n",
        "            logger.info(\"  Num examples: Unknown\")\n",
        "        logger.info(f\"  Batch size = {batch_size}\")\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        self.callback_handler.eval_dataloader = dataloader\n",
        "        # Do this before wrapping.\n",
        "        eval_dataset = dataloader.dataset\n",
        "\n",
        "        if is_torch_tpu_available():\n",
        "            dataloader = pl.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)\n",
        "\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = None\n",
        "\n",
        "        # Initialize containers\n",
        "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
        "        losses_host = None\n",
        "        preds_host = None\n",
        "        labels_host = None\n",
        "        classification_preds_host = None\n",
        "        classification_labels_host = None\n",
        "        # losses/preds/labels on CPU (final containers)\n",
        "        all_losses = None\n",
        "        all_preds = None\n",
        "        all_labels = None\n",
        "        all_preds_classification = None\n",
        "        all_labels_classification = None\n",
        "        # Will be useful when we have an iterable dataset so don't know its length.\n",
        "\n",
        "        observed_num_examples = 0\n",
        "        # Main evaluation loop\n",
        "        for step, inputs in enumerate(dataloader):\n",
        "            # Update the observed num examples\n",
        "            observed_batch_size = find_batch_size(inputs)\n",
        "            if observed_batch_size is not None:\n",
        "                observed_num_examples += observed_batch_size\n",
        "\n",
        "            # Prediction step\n",
        "            loss, logits, labels, classification_logits, classification_labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
        "\n",
        "            # Update containers on host\n",
        "            if loss is not None:\n",
        "                losses = self._nested_gather(loss.repeat(batch_size))\n",
        "                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
        "            if logits is not None:\n",
        "                logits = self._pad_across_processes(logits)\n",
        "                logits = self._nested_gather(logits)\n",
        "                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
        "            if labels is not None:\n",
        "                labels = self._pad_across_processes(labels)\n",
        "                labels = self._nested_gather(labels)\n",
        "                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
        "            if classification_logits is not None:\n",
        "                classification_logits = self._pad_across_processes(classification_logits)\n",
        "                classification_logits = self._nested_gather(classification_logits)\n",
        "                classification_preds_host = classification_logits if classification_preds_host is None else nested_concat(classification_preds_host, classification_logits, padding_index=-100)\n",
        "            if classification_labels is not None:\n",
        "                classification_labels = self._pad_across_processes(classification_labels)\n",
        "                classification_labels = self._nested_gather(classification_labels)\n",
        "                classification_labels_host = classification_labels if classification_labels_host is None else nested_concat(classification_labels_host, classification_labels, padding_index=-100)\n",
        "            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)\n",
        "\n",
        "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
        "            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:\n",
        "                if losses_host is not None:\n",
        "                    losses = nested_numpify(losses_host)\n",
        "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "                if preds_host is not None:\n",
        "                    logits = nested_numpify(preds_host)\n",
        "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "                if labels_host is not None:\n",
        "                    labels = nested_numpify(labels_host)\n",
        "                    all_labels = (\n",
        "                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "                    )\n",
        "                if classification_preds_host is not None:\n",
        "                    classification_logits = nested_numpify(classification_preds_host)\n",
        "                    all_preds_classification = classification_logits if all_preds_classification is None else nested_concat(all_preds_classification, classification_logits, padding_index=-100)\n",
        "                if classification_labels_host is not None:\n",
        "                    classification_labels = nested_numpify(classification_labels_host)\n",
        "                    all_labels_classification = (\n",
        "                        classification_labels if all_labels_classification is None else nested_concat(all_labels_classification, classification_labels, padding_index=-100)\n",
        "                    )\n",
        "\n",
        "                # Set back to None to begin a new accumulation\n",
        "                losses_host, preds_host, labels_host, classification_preds_host, classification_labels_host = None, None, None, None, None\n",
        "\n",
        "        if self.args.past_index and hasattr(self, \"_past\"):\n",
        "            # Clean the state at the end of the evaluation loop\n",
        "            delattr(self, \"_past\")\n",
        "\n",
        "        # Gather all remaining tensors and put them back on the CPU\n",
        "        if losses_host is not None:\n",
        "            losses = nested_numpify(losses_host)\n",
        "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "        if preds_host is not None:\n",
        "            logits = nested_numpify(preds_host)\n",
        "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "        if labels_host is not None:\n",
        "            labels = nested_numpify(labels_host)\n",
        "            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "        if classification_preds_host is not None:\n",
        "            classification_logits = nested_numpify(classification_preds_host)\n",
        "            all_preds_classification = classification_logits if all_preds_classification is None else nested_concat(all_preds_classification, classification_logits, padding_index=-100)\n",
        "        if classification_labels_host is not None:\n",
        "            classification_labels = nested_numpify(classification_labels_host)\n",
        "            all_labels_classification = classification_labels if all_labels_classification is None else nested_concat(all_labels_classification, classification_labels, padding_index=-100)\n",
        "\n",
        "        # Number of samples\n",
        "        if not isinstance(eval_dataset, IterableDataset):\n",
        "            num_samples = len(eval_dataset)\n",
        "        elif isinstance(eval_dataset, IterableDatasetShard):\n",
        "            num_samples = eval_dataset.num_examples\n",
        "        else:\n",
        "            num_samples = observed_num_examples\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses is not None:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds is not None:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels is not None:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "        if all_preds_classification is not None:\n",
        "            all_preds_classification = nested_truncate(all_preds_classification, num_samples)\n",
        "        if all_labels_classification is not None:\n",
        "            all_labels_classification = nested_truncate(all_labels_classification, num_samples)\n",
        "\n",
        "        # Metrics!\n",
        "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
        "            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
        "        else:\n",
        "            metrics = {}\n",
        "\n",
        "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
        "        metrics = denumpify_detensorize(metrics)\n",
        "\n",
        "        if all_losses is not None:\n",
        "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
        "\n",
        "        # Prefix all keys with metric_key_prefix + '_'\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
        "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return JointEvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples, classification_predictions=all_preds_classification, classification_label_ids=all_labels_classification)\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "        max_length: Optional[int] = None,\n",
        "        num_beams: Optional[int] = None,\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Run evaluation and returns metrics.\n",
        "\n",
        "        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
        "        (pass it to the init :obj:`compute_metrics` argument).\n",
        "\n",
        "        You can also subclass and override this method to inject custom behavior.\n",
        "\n",
        "        Args:\n",
        "            eval_dataset (:obj:`Dataset`, `optional`):\n",
        "                Pass a dataset if you wish to override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`,\n",
        "                columns not accepted by the ``model.forward()`` method are automatically removed. It must implement the\n",
        "                :obj:`__len__` method.\n",
        "            ignore_keys (:obj:`Lst[str]`, `optional`):\n",
        "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "                gathering predictions.\n",
        "            metric_key_prefix (:obj:`str`, `optional`, defaults to :obj:`\"eval\"`):\n",
        "                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
        "                \"eval_bleu\" if the prefix is \"eval\" (default)\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
        "            dictionary also contains the epoch number which comes from the training state.\n",
        "        \"\"\"\n",
        "        # memory metrics - must set up as early as possible\n",
        "        self._memory_tracker.start()\n",
        "\n",
        "        # From seq2seqTrainer:\n",
        "        self._max_length = max_length\n",
        "        self._num_beams = num_beams\n",
        "\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        start_time = time.time()\n",
        "\n",
        "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
        "        output = eval_loop(\n",
        "            eval_dataloader,\n",
        "            description=\"Evaluation\",\n",
        "            # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
        "            # self.args.prediction_loss_only\n",
        "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
        "            ignore_keys=ignore_keys,\n",
        "            metric_key_prefix=metric_key_prefix,\n",
        "        )\n",
        "\n",
        "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
        "        output.metrics.update(\n",
        "            speed_metrics(\n",
        "                metric_key_prefix,\n",
        "                start_time,\n",
        "                num_samples=output.num_samples,\n",
        "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.log(output.metrics)\n",
        "\n",
        "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
        "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
        "            xm.master_print(met.metrics_report())\n",
        "\n",
        "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n",
        "\n",
        "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        test_dataset: Dataset,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "        max_length: Optional[int] = None,\n",
        "        num_beams: Optional[int] = None,\n",
        "    ) -> JointPredictionOutput:\n",
        "        \"\"\"\n",
        "        Run prediction and returns predictions and potential metrics.\n",
        "\n",
        "        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n",
        "        will also return metrics, like in :obj:`evaluate()`.\n",
        "\n",
        "        Args:\n",
        "            test_dataset (:obj:`Dataset`):\n",
        "                Dataset to run the predictions on. If it is an :obj:`datasets.Dataset`, columns not accepted by the\n",
        "                ``model.forward()`` method are automatically removed. Has to implement the method :obj:`__len__`\n",
        "            ignore_keys (:obj:`Lst[str]`, `optional`):\n",
        "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "                gathering predictions.\n",
        "            metric_key_prefix (:obj:`str`, `optional`, defaults to :obj:`\"test\"`):\n",
        "                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
        "                \"test_bleu\" if the prefix is \"test\" (default)\n",
        "\n",
        "        .. note::\n",
        "\n",
        "            If your predictions or labels have different sequence length (for instance because you're doing dynamic\n",
        "            padding in a token classification task) the predictions will be padded (on the right) to allow for\n",
        "            concatenation into one array. The padding index is -100.\n",
        "\n",
        "        Returns: `NamedTuple` A namedtuple with the following keys:\n",
        "\n",
        "            - predictions (:obj:`np.ndarray`): The predictions on :obj:`test_dataset`.\n",
        "            - label_ids (:obj:`np.ndarray`, `optional`): The labels (if the dataset contained some).\n",
        "            - metrics (:obj:`Dict[str, float]`, `optional`): The potential dictionary of metrics (if the dataset\n",
        "              contained labels).\n",
        "        \"\"\"\n",
        "        # memory metrics - must set up as early as possible\n",
        "        self._memory_tracker.start()\n",
        "\n",
        "        # From seq2seqTrainer:\n",
        "        self._max_length = max_length\n",
        "        self._num_beams = num_beams\n",
        "\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "        start_time = time.time()\n",
        "\n",
        "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
        "        output = eval_loop(\n",
        "            test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix\n",
        "        )\n",
        "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
        "        output.metrics.update(\n",
        "            speed_metrics(\n",
        "                metric_key_prefix,\n",
        "                start_time,\n",
        "                num_samples=output.num_samples,\n",
        "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
        "\n",
        "        return JointPredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics, classification_predictions=output.classification_predictions, classification_label_ids=output.classification_label_ids)\n",
        "\n",
        "\n",
        "\n",
        "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\n",
        "                f\"Tensor need to be padded to `max_length={max_length}` but no tokenizer was passed when creating \"\n",
        "                \"this `Trainer`. Make sure to create your `Trainer` with the appropriate tokenizer.\"\n",
        "            )\n",
        "        # If PAD token is not defined at least EOS token has to be defined\n",
        "        pad_token_id = (\n",
        "            self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        padded_tensor = pad_token_id * torch.ones(\n",
        "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
        "        )\n",
        "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
        "        return padded_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ii_khjb2wzY"
      },
      "source": [
        "# 7. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6OOYhtA2zEx"
      },
      "source": [
        "from torch.nn import Softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtGzIjqJ2zpv"
      },
      "source": [
        "# Copy saved model to this session\n",
        "!mkdir /content/results\n",
        "!cp ../drive/MyDrive/Thesis/System_Development/results/Result3.zip /content/results/\n",
        "%cd ../results\n",
        "!unzip Result3.zip\n",
        "%cd ../transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxNFxMGb3H3g"
      },
      "source": [
        "# ../results/tst-summarization/ contains the output from saving the model during the training section\n",
        "model = BartForJointPrediction.from_pretrained('../results/tst-summarization/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7qJsIRO3QYS"
      },
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    adafactor=False,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    adam_epsilon=1e-08,\n",
        "    dataloader_drop_last=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=True,\n",
        "    ddp_find_unused_parameters=None,\n",
        "    debug=[],\n",
        "    deepspeed=None,\n",
        "    disable_tqdm=False,\n",
        "    do_eval=True,\n",
        "    do_predict=False,\n",
        "    do_train=False,\n",
        "    eval_accumulation_steps=None,\n",
        "    eval_steps=500,\n",
        "    evaluation_strategy='no',\n",
        "    fp16=False,\n",
        "    fp16_backend='auto',\n",
        "    fp16_full_eval=False,\n",
        "    fp16_opt_level='O1',\n",
        "    gradient_accumulation_steps=1,\n",
        "    greater_is_better=None,\n",
        "    group_by_length=False,\n",
        "    ignore_data_skip=False,\n",
        "    label_names=None,\n",
        "    label_smoothing_factor=0.0,\n",
        "    learning_rate=5e-05,\n",
        "    length_column_name='length',\n",
        "    load_best_model_at_end=False,\n",
        "    local_rank=-1,\n",
        "    # log_level=-1,\n",
        "    # log_level_replica=-1,\n",
        "    log_on_each_node=True,\n",
        "    logging_dir='/tmp/tst-summarization/runs/Jul04_02-41-44_9ee3aa777e7a',\n",
        "    logging_first_step=False,\n",
        "    logging_steps=500,\n",
        "    logging_strategy='steps',\n",
        "    lr_scheduler_type='linear',\n",
        "    max_grad_norm=1.0,\n",
        "    max_steps=-1,\n",
        "    metric_for_best_model=None,\n",
        "    # mp_parameters=,\n",
        "    no_cuda=False,\n",
        "    num_train_epochs=3.0,\n",
        "    output_dir='/tmp/tst-summarization',\n",
        "    overwrite_output_dir=True,\n",
        "    past_index=-1,\n",
        "    per_device_eval_batch_size=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    predict_with_generate=True,\n",
        "    prediction_loss_only=False,\n",
        "    push_to_hub=False,\n",
        "    push_to_hub_model_id='tst-summarization',\n",
        "    push_to_hub_organization=None,\n",
        "    push_to_hub_token=None,\n",
        "    remove_unused_columns=True,\n",
        "    report_to=['tensorboard'],\n",
        "    resume_from_checkpoint=None,\n",
        "    run_name='/tmp/tst-summarization',\n",
        "    save_on_each_node=False,\n",
        "    save_steps=10000,\n",
        "    save_strategy='steps',\n",
        "    save_total_limit=None,\n",
        "    seed=42,\n",
        "    sharded_ddp=[],\n",
        "    skip_memory_metrics=True,\n",
        "    sortish_sampler=False,\n",
        "    tpu_metrics_debug=False,\n",
        "    tpu_num_cores=None,\n",
        "    use_legacy_prediction_loop=False,\n",
        "    warmup_ratio=0.0,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWAr8DB3Vdw"
      },
      "source": [
        "trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=None,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdwjf8Yi3XtT"
      },
      "source": [
        "predictions = trainer.predict(\n",
        "            val_dataset, max_length=100, num_beams=4, metric_key_prefix=\"eval\"\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6DmBFsl3Zny"
      },
      "source": [
        "predictions.metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsA88vEC3e24"
      },
      "source": [
        "Evaluation of Classification Accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VFpsgen3brG"
      },
      "source": [
        "# Prepare predictions\n",
        "classification_logits = torch.from_numpy(predictions.classification_predictions)\n",
        "final_layer = Softmax(dim=1)\n",
        "classification_preds = torch.argmax(final_layer(classification_logits), dim=1)\n",
        "classification_preds = classification_preds.numpy()\n",
        "\n",
        "# Prepare labels\n",
        "gold_label = np.array(val_dataset['classification_labels'])\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = np.mean(classification_preds == gold_label)\n",
        "\n",
        "# Compute filtered accuracy\n",
        "idx = gold_label != 2\n",
        "classification_preds_filtered = classification_preds[idx]\n",
        "gold_label_filtered = gold_label[idx]\n",
        "\n",
        "filtered_accuracy = np.mean(classification_preds_filtered == gold_label_filtered)\n",
        "\n",
        "print(\"FULL\")\n",
        "print(accuracy)\n",
        "print(\"WITHOUT CLASS 2\")\n",
        "print(filtered_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dxpcHn53hMC"
      },
      "source": [
        "Evaluation of Explanations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbL4airy3d7o"
      },
      "source": [
        "summary = torch.from_numpy(predictions.predictions)\n",
        "decoded_summaries = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqYEScLt3yTR"
      },
      "source": [
        "Save Results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn_MHf-l3oOI"
      },
      "source": [
        "df_val['pred_label'] = classification_preds\n",
        "df_val['pred_explanation'] = decoded_summaries\n",
        "df_val.to_csv(\"model_eFEVER_data_eFEVER.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ImO7LQk31w9"
      },
      "source": [
        "# 8. Temperature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1WCdl-J3t8n"
      },
      "source": [
        "training_len = 9999\n",
        "device = 'cuda:0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOe2ao9S3_HF"
      },
      "source": [
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nY05Rb24BKS"
      },
      "source": [
        "!mkdir /content/results\n",
        "!cp ../drive/MyDrive/Thesis/System_Development/results/Result3.zip /content/results/\n",
        "%cd ../results\n",
        "!unzip Result3.zip\n",
        "%cd ../transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dln_t_kf4Inz"
      },
      "source": [
        "# ../results/tst-summarization/ contains the output from saving the model during the training section\n",
        "model = BartForJointPrediction.from_pretrained('../results/tst-summarization/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbMaN81D4Ll0"
      },
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    adafactor=False,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    adam_epsilon=1e-08,\n",
        "    dataloader_drop_last=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=True,\n",
        "    ddp_find_unused_parameters=None,\n",
        "    debug=[],\n",
        "    deepspeed=None,\n",
        "    disable_tqdm=False,\n",
        "    do_eval=True,\n",
        "    do_predict=False,\n",
        "    do_train=False,\n",
        "    eval_accumulation_steps=None,\n",
        "    eval_steps=500,\n",
        "    evaluation_strategy='no',\n",
        "    fp16=False,\n",
        "    fp16_backend='auto',\n",
        "    fp16_full_eval=False,\n",
        "    fp16_opt_level='O1',\n",
        "    gradient_accumulation_steps=1,\n",
        "    greater_is_better=None,\n",
        "    group_by_length=False,\n",
        "    ignore_data_skip=False,\n",
        "    label_names=None,\n",
        "    label_smoothing_factor=0.0,\n",
        "    learning_rate=5e-05,\n",
        "    length_column_name='length',\n",
        "    load_best_model_at_end=False,\n",
        "    local_rank=-1,\n",
        "    # log_level=-1,\n",
        "    # log_level_replica=-1,\n",
        "    log_on_each_node=True,\n",
        "    logging_dir='/tmp/tst-summarization/runs/Jul04_02-41-44_9ee3aa777e7a',\n",
        "    logging_first_step=False,\n",
        "    logging_steps=500,\n",
        "    logging_strategy='steps',\n",
        "    lr_scheduler_type='linear',\n",
        "    max_grad_norm=1.0,\n",
        "    max_steps=-1,\n",
        "    metric_for_best_model=None,\n",
        "    # mp_parameters=,\n",
        "    no_cuda=False,\n",
        "    num_train_epochs=3.0,\n",
        "    output_dir='/tmp/tst-summarization',\n",
        "    overwrite_output_dir=True,\n",
        "    past_index=-1,\n",
        "    per_device_eval_batch_size=4,\n",
        "    per_device_train_batch_size=4,\n",
        "    predict_with_generate=True,\n",
        "    prediction_loss_only=False,\n",
        "    push_to_hub=False,\n",
        "    push_to_hub_model_id='tst-summarization',\n",
        "    push_to_hub_organization=None,\n",
        "    push_to_hub_token=None,\n",
        "    remove_unused_columns=True,\n",
        "    report_to=['tensorboard'],\n",
        "    resume_from_checkpoint=None,\n",
        "    run_name='/tmp/tst-summarization',\n",
        "    save_on_each_node=False,\n",
        "    save_steps=10000,\n",
        "    save_strategy='steps',\n",
        "    save_total_limit=None,\n",
        "    seed=42,\n",
        "    sharded_ddp=[],\n",
        "    skip_memory_metrics=True,\n",
        "    sortish_sampler=False,\n",
        "    tpu_metrics_debug=False,\n",
        "    tpu_num_cores=None,\n",
        "    use_legacy_prediction_loop=False,\n",
        "    warmup_ratio=0.0,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RD2tkuW4R2Z"
      },
      "source": [
        "trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=None,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBkhHkU_4Ty4"
      },
      "source": [
        "predictions = trainer.predict(\n",
        "            val_dataset, max_length=100, num_beams=4, metric_key_prefix=\"eval\"\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlR944Nd4WnA"
      },
      "source": [
        "Explanations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLrzVPz54WLx"
      },
      "source": [
        "summary = torch.from_numpy(predictions.predictions)\n",
        "decoded_summaries = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary]\n",
        "df_val['pred_explanation'] = decoded_summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPjZgw8x4dXA"
      },
      "source": [
        "Preliminary Setup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qaT2E-v4f41"
      },
      "source": [
        "# Split validation set into training/testing subsets\n",
        "training_logits = predictions.classification_predictions[:training_len]\n",
        "testing_logits = predictions.classification_predictions[training_len:]\n",
        "\n",
        "training_labels = np.array(val_dataset['classification_labels'])[:training_len]\n",
        "testing_labels = np.array(val_dataset['classification_labels'])[training_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIZWzIVR4irR"
      },
      "source": [
        "Evaluation Helper Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J00XuNZn4kch"
      },
      "source": [
        "def calc_bins(preds, labels_oneh):\n",
        "  # Assign each prediction to a bin\n",
        "  num_bins = 10\n",
        "  bins = np.linspace(0.1, 1, num_bins)\n",
        "  binned = np.digitize(preds, bins)\n",
        "\n",
        "  # Save the accuracy, confidence and size of each bin\n",
        "  bin_accs = np.zeros(num_bins)\n",
        "  bin_confs = np.zeros(num_bins)\n",
        "  bin_sizes = np.zeros(num_bins)\n",
        "\n",
        "  for bin in range(num_bins):\n",
        "    bin_sizes[bin] = len(preds[binned == bin])\n",
        "    if bin_sizes[bin] > 0:\n",
        "      bin_accs[bin] = (labels_oneh[binned==bin]).sum() / bin_sizes[bin]\n",
        "      bin_confs[bin] = (preds[binned==bin]).sum() / bin_sizes[bin]\n",
        "\n",
        "  return bins, binned, bin_accs, bin_confs, bin_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYL1mnVe4lq3"
      },
      "source": [
        "def get_metrics(preds, labels_oneh):\n",
        "  ECE = 0\n",
        "  MCE = 0\n",
        "  bins, _, bin_accs, bin_confs, bin_sizes = calc_bins(preds, labels_oneh)\n",
        "\n",
        "  for i in range(len(bins)):\n",
        "    abs_conf_dif = abs(bin_accs[i] - bin_confs[i])\n",
        "    ECE += (bin_sizes[i] / sum(bin_sizes)) * abs_conf_dif\n",
        "    MCE = max(MCE, abs_conf_dif)\n",
        "\n",
        "  return ECE, MCE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRzaQbrw4pBL"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "import matplotlib\n",
        "\n",
        "def draw_reliability_graph(preds, labels_oneh):\n",
        "  ECE, MCE = get_metrics(preds, labels_oneh)\n",
        "  bins, _, bin_accs, _, _ = calc_bins(preds, labels_oneh)\n",
        "\n",
        "  font = {'family' : 'normal',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 16}\n",
        "  matplotlib.rc('font', **font)\n",
        "\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  ax = fig.gca()\n",
        "\n",
        "  # x/y limits\n",
        "  ax.set_xlim(0, 1.05)\n",
        "  ax.set_ylim(0, 1)\n",
        "\n",
        "  # x/y labels\n",
        "  plt.xlabel('Confidence')\n",
        "  plt.ylabel('Accuracy')\n",
        "\n",
        "  # Create grid\n",
        "  ax.set_axisbelow(True) \n",
        "  ax.grid(color='gray', linestyle='dashed')\n",
        "\n",
        "  # Error bars\n",
        "  plt.bar(bins, bins,  width=0.1, alpha=0.3, edgecolor='black', color='r', hatch='\\\\')\n",
        "\n",
        "  # Draw bars and identity line\n",
        "  plt.bar(bins, bin_accs, width=0.1, alpha=1, edgecolor='black', color='b')\n",
        "  plt.plot([0,1],[0,1], '--', color='gray', linewidth=2)\n",
        "\n",
        "  # Equally spaced axes\n",
        "  plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "  # ECE and MCE legend\n",
        "  ECE_patch = mpatches.Patch(color='green', label='ECE = {:.2f}%'.format(ECE*100))\n",
        "  MCE_patch = mpatches.Patch(color='red', label='MCE = {:.2f}%'.format(MCE*100))\n",
        "  plt.legend(handles=[ECE_patch, MCE_patch])\n",
        "\n",
        "  #plt.show()\n",
        "  \n",
        "  plt.savefig('calibrated_network.png', bbox_inches='tight')\n",
        "\n",
        "#draw_reliability_graph(preds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gFLzB384tzX"
      },
      "source": [
        "## Evaluation Before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-OzCeD24tL8"
      },
      "source": [
        "classification_logits = torch.from_numpy(testing_logits)\n",
        "final_layer = nn.Softmax(dim=1)\n",
        "class_softmax = final_layer(classification_logits)\n",
        "class_softmax_flat = np.array(class_softmax).flatten()\n",
        "\n",
        "gold_label = testing_labels\n",
        "gold_label = torch.from_numpy(gold_label)\n",
        "labels_oneh = torch.nn.functional.one_hot(gold_label, num_classes=3)\n",
        "labels_oneh = np.array(labels_oneh).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jybslae847zx"
      },
      "source": [
        "Save Classification:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxc3JIfw42Nb"
      },
      "source": [
        "classification_preds = torch.argmax(final_layer(classification_logits), dim=1)\n",
        "classification_preds = classification_preds.numpy()\n",
        "filler = np.ones(training_len) * -1 # use -1 for subset of validation data used to train temperature parameter\n",
        "classification_preds = np.concatenate((filler, classification_preds))\n",
        "df_val['pred_label_orig'] = classification_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk4ZcCLB5F0P"
      },
      "source": [
        "Reliability Diagram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBL5XwOw5HB0"
      },
      "source": [
        "draw_reliability_graph(class_softmax_flat, labels_oneh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PLzks2E5IK_"
      },
      "source": [
        "## Apply Temperature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TZMZQ2A5L_7"
      },
      "source": [
        "def T_scaling(logits, args):\n",
        "  temperature = args.get('temperature', None)\n",
        "  return torch.div(logits, temperature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iwOp4A25PzQ"
      },
      "source": [
        "temperature = nn.Parameter(torch.ones(1).cuda())\n",
        "args = {'temperature': temperature}\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Removing strong_wolfe line search results in jump after 50 epochs\n",
        "optimizer = optim.LBFGS([temperature], lr=0.001, max_iter=10000, line_search_fn='strong_wolfe')\n",
        "\n",
        "logits_list = []\n",
        "labels_list = []\n",
        "temps = []\n",
        "losses = []\n",
        "\n",
        "logits_list = torch.from_numpy(training_logits).to(device)\n",
        "labels_list = torch.from_numpy(training_labels).to(device)\n",
        "\n",
        "def _eval():\n",
        "  loss = criterion(T_scaling(logits_list, args), labels_list)\n",
        "  loss.backward()\n",
        "  temps.append(temperature.item())\n",
        "  losses.append(loss)\n",
        "  return loss\n",
        "\n",
        "\n",
        "optimizer.step(_eval)\n",
        "\n",
        "print('Final T_scaling factor: {:.2f}'.format(temperature.item()))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(list(range(len(temps))), temps)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(list(range(len(losses))), losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hyRlNgD5WZ2"
      },
      "source": [
        "## Evaluation After"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr7pund85YV9"
      },
      "source": [
        "args = {'temperature': temperature}\n",
        "\n",
        "classification_logits = torch.from_numpy(testing_logits)\n",
        "classification_logits = classification_logits.to(device)\n",
        "classification_logits = T_scaling(classification_logits, args)\n",
        "final_layer = nn.Softmax(dim=1)\n",
        "class_softmax = final_layer(classification_logits)\n",
        "class_softmax = class_softmax.cpu().detach().numpy()\n",
        "class_softmax_flat = class_softmax.flatten()\n",
        "\n",
        "gold_label = testing_labels\n",
        "gold_label = torch.from_numpy(gold_label)\n",
        "labels_oneh = torch.nn.functional.one_hot(gold_label, num_classes=3)\n",
        "labels_oneh = np.array(labels_oneh).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O_B26bz5bcx"
      },
      "source": [
        "classification_preds = torch.from_numpy(class_softmax)\n",
        "classification_preds = torch.argmax(classification_preds, dim=1)\n",
        "classification_preds = classification_preds.numpy()\n",
        "filler = np.ones(training_len) * -1\n",
        "classification_preds = np.concatenate((filler, classification_preds))\n",
        "df_val['pred_label_final'] = classification_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95iFl115dUW"
      },
      "source": [
        "draw_reliability_graph(class_softmax_flat, labels_oneh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPYyJLTK5j2Q"
      },
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osRMgayk5mfh"
      },
      "source": [
        "np.savetxt(\"softmax.csv\", class_softmax, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1GKYgMq6YQ9"
      },
      "source": [
        "filler = np.ones(training_len) * -1 # use -1 for subset of validation data used to train temperature parameter\n",
        "classification_probs = np.concatenate((filler, class_softmax[:,0]))\n",
        "df_val['prob_class_0'] = classification_probs\n",
        "\n",
        "filler = np.ones(training_len) * -1 # use -1 for subset of validation data used to train temperature parameter\n",
        "classification_probs = np.concatenate((filler, class_softmax[:,1]))\n",
        "df_val['prob_class_1'] = classification_probs\n",
        "\n",
        "filler = np.ones(training_len) * -1 # use -1 for subset of validation data used to train temperature parameter\n",
        "classification_probs = np.concatenate((filler, class_softmax[:,2]))\n",
        "df_val['prob_class_2'] = classification_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRd3hlqK52yS"
      },
      "source": [
        "df_val.to_csv(\"val_preds_temperature_scaling.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}